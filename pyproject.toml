[project]
name = "table-evaluator"
version = "1.9.0"
# Alternative: Use setuptools_scm for automatic versioning
# dynamic = ["version"]
description = "A package to evaluate how close a synthetic data set is to real data."
authors = [
    { name = "Bauke Brenninkmeijer", email = "bauke.brenninkmeijer@gmail.com" }
]
license = "MIT"
requires-python = ">=3.10"
readme = "README.md"
keywords = ['Table-evaluation', 'synthetic-data', 'data-generation', 'data', 'generation', 'data-evaluation']
classifiers = [
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: 3 :: Only',
    "Operating System :: OS Independent",
]
dependencies = [
    "pyarrow==17.0.0",
    "scikit-learn==1.5.1",
    "tqdm==4.66.5",
    "matplotlib==3.9.2",
    "numpy==1.26.4",
    "seaborn==0.13.2",
    "pandas==2.2.2",
    "scipy==1.12.0",
    "pre-commit==3.8.0",
    "ipython==8.28.0",
    "pytest==8.3.2",
    "sphinx==7.3.1",
    "pytest-cov==4.0.0",
    "pytest-mock==3.14.0",
    "m2r2==0.3.3.post2",
    "setuptools",
    "sphinx-rtd-theme==2.0.0",
    "tomli>=1.2.0;python_version<'3.11'",
]


[project.optional-dependencies]
dev = [
    "build>=1.2.2.post1",
    "ipykernel>=6.29.5",
    "ipywidgets>=8.1.7",
    "psutil>=5.9.0",
    "pytest-benchmark>=4.0.0",
]
polars = [
    "polars>=0.20.0,<2.0.0",
]
all = [
    "polars>=0.20.0,<2.0.0",
    "build>=1.2.2.post1",
    "ipykernel>=6.29.5",
    "ipywidgets>=8.1.7",
    "psutil>=5.9.0",
    "pytest-benchmark>=4.0.0",
]


# Alternative setuptools_scm configuration (commented out)
# [tool.setuptools_scm]
# write_to = "table_evaluator/_version.py"

[project.urls]
"Homepage" = "https://github.com/Baukebrenninkmeijer/Table-Evaluator"
"Documentation" = "https://baukebrenninkmeijer.github.io/table-evaluator/index.html"

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"


[tool.setuptools.packages.find]
where = ["."]
include = ["table_evaluator*"]


[tool.pytest.ini_options]
minversion = "6.0"
addopts = "-ra -q --strict-markers --cov=table_evaluator --cov-report=term-missing --cov-report=html"
testpaths = ["tests"]
norecursedirs = ["benchmarks", ".git", ".tox", "dist", "*.egg"]
markers = [
    "data_loading: marks tests as data loading benchmarks",
    "evaluation: marks tests as evaluation benchmarks",
    "data_conversion: marks tests as data conversion benchmarks",
    "association: marks tests as association metric benchmarks"
]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false
disallow_incomplete_defs = false
check_untyped_defs = true
disallow_untyped_decorators = false
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = ["*"]
ignore_missing_imports = true

[tool.coverage.run]
source = ["table_evaluator"]
omit = ["*/tests/*"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]

[tool.bandit]
exclude_dirs = ["tests", "docs"]
skips = ["B101", "B601"]

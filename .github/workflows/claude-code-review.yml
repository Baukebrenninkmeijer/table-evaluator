name: Claude Code Review

on:
  pull_request:
    types: [opened, synchronize]
    # Optional: Only run on specific file changes
    paths:
      - "table_evaluator/**/*.py"
      - "tests/**/*.py"
      - "pyproject.toml"
      - "requirements*.txt"
      - "setup.py"

jobs:
  claude-review:
    # Optional: Filter by PR author
    # if: |
    #   github.event.pull_request.user.login == 'external-contributor' ||
    #   github.event.pull_request.user.login == 'new-developer' ||
    #   github.event.pull_request.author_association == 'FIRST_TIME_CONTRIBUTOR'

    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Run Claude Code Review
        id: claude-review
        timeout-minutes: 10
        uses: anthropics/claude-code-action@beta
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

          # Optional: Specify model (defaults to Claude Sonnet 4, uncomment for Claude Opus 4)
          # model: "claude-opus-4-20250514"

          # Direct prompt for automated review (no @claude mention needed)
          direct_prompt: |
            Please review this Python pull request for the table-evaluator library and provide feedback on:

            ## purpose evaluation:
            - Does the implementation make sense given the task?
            - Is this the best way to approach the problem?
            - Is there any existing solution to this problem that is different from ours?

            ## Code Quality & Standards:
            - PEP 8 compliance and Black formatting
            - Type hints usage and accuracy
            - Docstring quality and completeness
            - Import organization and dependencies

            ## Architecture & Design:
            - Adherence to existing architectural patterns
            - Separation of concerns (metrics vs evaluators vs API)
            - Backward compatibility considerations
            - Plugin architecture integration (if applicable)

            ## Performance & Efficiency:
            - NumPy/Pandas/Polars usage optimization
            - Memory management for large datasets
            - Computational complexity considerations
            - Caching opportunities

            ## Testing & Reliability:
            - Test coverage completeness
            - Edge case handling
            - Error handling and logging
            - Integration test quality

            ## Security & Privacy:
            - Input validation and sanitization
            - Privacy considerations for data evaluation
            - Security best practices

            ## Library-Specific Concerns:
            - Support for both Pandas and Polars backends
            - Advanced metrics implementation accuracy
            - Visualization integration
            - Configuration management

            Be constructive, specific, and provide actionable feedback with code examples when helpful.

          # Optional: Use sticky comments to make Claude reuse the same comment on subsequent pushes to the same PR
          use_sticky_comment: true

          # Optional: Customize review based on file types
          # direct_prompt: |
          #   Review this PR focusing on:
          #   - For TypeScript files: Type safety and proper interface usage
          #   - For API endpoints: Security, input validation, and error handling
          #   - For React components: Performance, accessibility, and best practices
          #   - For tests: Coverage, edge cases, and test quality

          # Optional: Different prompts for different authors
          # direct_prompt: |
          #   ${{ github.event.pull_request.author_association == 'FIRST_TIME_CONTRIBUTOR' &&
          #   'Welcome! Please review this PR from a first-time contributor. Be encouraging and provide detailed explanations for any suggestions.' ||
          #   'Please provide a thorough code review focusing on our coding standards and best practices.' }}

          # Optional: Add specific tools for running tests or linting
          allowed_tools: "Bash(python -m pytest*),Bash(python -m ruff*),Bash(python -m black*),Bash(python -m bandit*),Bash(make test),Bash(make lint)"

          # Optional: Skip review for certain conditions
          # if: |
          #   !contains(github.event.pull_request.title, '[skip-review]') &&
          #   !contains(github.event.pull_request.title, '[WIP]')
